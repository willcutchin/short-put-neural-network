import numpy as np
import pandas as pd
from Data_Fixing_for_RNN import *
from Data import *
from Data_Fixing_for_RNN import *

#make alpha .01
#data = np.array(data)
#m, n = data.shape
#m is the amount of rows we have
#n is the amount of features +1
def intit_initialVal(sizeOf):
    #intialize param
    
    weight1 = np.random.rand(5, sizeOf-1) -.5 #We set it eqaul to 10, 100 beacuse it is going to generate random num between -.5 and .5 for every element in the arr
    bias1 = np.random.rand(5,1) -.5 #This is a random bais for the nueral network between -.5 and .5
    weight2 = np.random.rand(5, 5) -.5 #The next wieght is only between 10 and 10 because it ends up being our hidden layer which only has 10 neurons
    bias2 = np.random.rand(5,1) -.5 #This does the same thing as bais 1
    return weight1, bias1, weight2, bias2

def ReLu(z):
    #This is the activation function
    return np.maximum(0, z)

def softMax(z):
    return np.exp(z)/np.sum(np.exp(z))


def forwardProp(weight1, bias1, weight2, bias2, x):
    Z1 = weight1.dot(x) + bias1 #Z1 is the unactivate hidden layer that we are intializing, we just creeate a weight and bais from intitVal
    A1 = ReLu(Z1) #Rectified linear unit is our activation function
    Z2 = weight2.dot(A1) + bias2 # second unactivate layer
    A2 = softMax(A1) #second activation function
    return Z1, A1, Z2, A2

def changeY(y):
    #
    newY = np.zeros((y.size, int(y.max() +1)))
    newY[np.arange(y.size), y] = 1
    newY = newY.T
    return newY

def derivOfRelu(Z):
    if(Z > 0):
        return 1
    return 0


def backProp(Z1, A1, Z2, A2, weight2, X, Y):
    #Back prop is a way that we go back to the layer that we were just at. This helps the bais and wieght change and get us a much clearer answer
    newY = changeY(Y) #Gives us a new y-value so this is not linear
    dz2 = A2 - newY #This is the error of the second layer or what we would like to 
    dweight2 = (1/Y.size) * dz2.dot(A1.T)
    dbias2 = (1/Y.size) * np.sum(dz2,2)
    dz1 = weight2.T.dot(dz2) * derivOfRelu(Z1)
    dweight1 = (1/Y.size) * dz1.dot(X.T)
    dbias1 = (1/Y.size) * np.sum(dz1,2)

    return dweight1, dbias1, dweight2, dbias2

def update(alpha, weight1, bias1, weight2, bias2, dweight1, dbias1, dweight2, dbias2):
    weight1 = weight1 - (alpha * dweight1)
    bias1 = bias1 - (alpha * dbias1)
    weight2 = weight2 - (alpha * dbias2)
    bias2 = bias2 - (alpha * dbias2)
    return weight1, bias1, weight2, bias2

def predictions(A2):
    return np.argmax(A2, 0)

def accuracy(predict, y):
    print(predict, y)
    return np.sum(predict == y)/y.size

def gradient(x, y, iter, alpha, sizeOf):
    weight1, bias1, weight2, bias2 = intit_initialVal(sizeOf)
    for i in range(iter):
        Z1, A1, Z2, A2 = forwardProp(weight1, bias1, weight2, bias2, x)
        dweight1, dbias1, dweight2, dbias2 = backProp(Z1, A1, Z2, A2, weight2, x, y)
        weight1, bias1, weight2, bias2 = update(alpha, weight1, bias1, weight2, bias2, dweight1, dbias1, dweight2, dbias2)
        if i % 10 == 0:
            print("iteration " + i +"\n"+"Accuracy")
            print(accuracy(predictions(A2),y))
    return weight1, bias1, weight2, bias2





cleaning = inputs()

data_dev_test, Y_dev, X_dev, data_train, Y_train, X_train, sizeOf = inputs.cleaning()

gradient(X_train, Y_train, 300, .01, len(X_train)+1)
    
